{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f8bd84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Load the AHAP file and modify with new merged audio\\n    with open(sound_file_path.split(\\'.\\', 1)[0] + \\'_combined.ahap\\', \\'r\\') as file:\\n        data = json.load(file)\\n\\n    # Define the new event to be added\\n    new_event = {\\n        \"Event\": {\\n            \"Time\": 0.0,\\n            \"EventType\":\"AudioCustom\",\\n            \"EventWaveformPath\":output_path,\\n              \"EventParameters\":\\n              [\\n                  {\"ParameterID\":\"AudioVolume\",\"ParameterValue\":0.75}\\n              ]\\n        }\\n    }\\n\\n    # Append the new event to the \"Pattern\" list\\n    data[\"Pattern\"].append(new_event)\\n\\n    # Save the modified JSON data back to the file\\n    with open(sound_file_path.split(\\'.\\', 1)[0] + \\'_combined.ahap\\', \\'w\\') as file:\\n        json.dump(data, file, indent=4)\\n    \\n    print(\"New event added successfully.\")\\n\\n\\n    # Overlay background sounds based on detected keywords\\n    for keyword in detected_keywords:\\n        background_file = keywords_to_sounds[keyword]\\n        print(background_file)\\n        audio_segment = overlay_background_sound(audio_segment, background_file)\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import time\n",
    "import json\n",
    "from AHAPpy import generate_ahap as ga\n",
    "import os.path\n",
    "\n",
    "\n",
    "def pitch_shift(audio, sample_rate, n_steps):\n",
    "    return librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=n_steps)\n",
    "\n",
    "def time_stretch(audio, rate):\n",
    "    return librosa.effects.time_stretch(audio, rate=rate)\n",
    "\n",
    "def change_amplitude(audio_segment, db_change):\n",
    "    return audio_segment + db_change\n",
    "\n",
    "def apply_equalizer(audio_segment, low_gain, mid_gain, high_gain):\n",
    "    low = audio_segment.low_pass_filter(200).apply_gain(low_gain)\n",
    "    mid = audio_segment.high_pass_filter(200).low_pass_filter(2000).apply_gain(mid_gain)\n",
    "    high = audio_segment.high_pass_filter(2000).apply_gain(high_gain)\n",
    "    return low.overlay(mid).overlay(high)\n",
    "\n",
    "def detect_keywords(audio_file, keywords, retries=3, delay=5):\n",
    "    recognizer = sr.Recognizer()\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with sr.AudioFile(audio_file) as source:\n",
    "                audio = recognizer.record(source)\n",
    "                transcript = recognizer.recognize_google(audio)\n",
    "                print(transcript)\n",
    "                detected_keywords = [kw for kw in keywords if kw in transcript.lower()]\n",
    "                return detected_keywords\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Request error: {e}. Attempt {attempt + 1}/{retries}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return []\n",
    "        except sr.UnknownValueError:\n",
    "            return []\n",
    "\n",
    "def adjust_audio_duration(audio, duration_seconds):\n",
    "    # Calculate the desired duration in milliseconds\n",
    "    desired_duration_ms = duration_seconds * 1000\n",
    "    if len(audio) > desired_duration_ms:\n",
    "        # Trim the audio if it's longer than the desired duration\n",
    "        audio = audio[:desired_duration_ms]\n",
    "    elif len(audio) < desired_duration_ms:\n",
    "        # Loop the audio if it's shorter than the desired duration\n",
    "        loop_count = desired_duration_ms // len(audio)\n",
    "        remaining_duration = desired_duration_ms % len(audio)\n",
    "        audio = audio * loop_count + audio[:remaining_duration]\n",
    "    print(\"Successfully adjusted audio.\")\n",
    "    return audio\n",
    "\n",
    "def overlay_background_sound(audio_segment, background_file, volume=-10):\n",
    "    background = AudioSegment.from_file(background_file).apply_gain(volume)\n",
    "    return audio_segment.overlay(background, loop=True)\n",
    "\n",
    "def process_audio(speech_file_path, sound_file_paths, duration_seconds, pitch_steps, time_stretch_rate, amp_change, eq_settings, keywords_to_sounds):\n",
    "    \n",
    "    #output_path= str(os.path.dirname(speech_file_path)) + \"/\" + os.path.splitext(os.path.basename(speech_file_path))[0] + \"-\" + os.path.splitext(os.path.basename(sound_file_path))[0] + \"-output.wav\"\n",
    "    output_background_path=\"/Users/user/Downloads/embrace_audio/Output-background.wav\"\n",
    "    output_path=\"/Users/user/Downloads/embrace_audio/Output.wav\"\n",
    "    print (\"output_path:\", output_path)\n",
    "    \n",
    "    # Load audio file\n",
    "    audio, sample_rate = librosa.load(speech_file_path, sr=None)\n",
    "\n",
    "    # Pitch Shifting\n",
    "    shifted_audio = pitch_shift(audio, sample_rate, pitch_steps)\n",
    "\n",
    "    # Time Stretching\n",
    "    stretched_audio = time_stretch(shifted_audio, time_stretch_rate)\n",
    "\n",
    "    # Convert to pydub AudioSegment for amplitude and EQ\n",
    "    audio_segment = AudioSegment(\n",
    "        (stretched_audio * 32767).astype(np.int16).tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    # Amplitude Manipulation\n",
    "    audio_segment = change_amplitude(audio_segment, amp_change)\n",
    "\n",
    "    # Spectrum Manipulation (Basic EQ)\n",
    "    eq_audio_segment = apply_equalizer(audio_segment, *eq_settings)\n",
    "    \n",
    "    # Detect keywords in the audio\n",
    "    # detected_keywords = detect_keywords(file_path, keywords_to_sounds.keys())\n",
    "\n",
    "    # Load the original audio\n",
    "    audio_segment = eq_audio_segment\n",
    "\n",
    "    sound_segment = AudioSegment.from_file(sound_file_paths[0])\n",
    "    sound_segment = adjust_audio_duration(sound_segment, duration_seconds)\n",
    "    sound_index = 1\n",
    "    \n",
    "    while sound_index < len(sound_file_paths):\n",
    "        background = AudioSegment.from_file(sound_file_paths[sound_index])\n",
    "        background = adjust_audio_duration(background, duration_seconds)\n",
    "        sound_segment = sound_segment.overlay(background, loop=True)\n",
    "        sound_index += 1\n",
    "\n",
    "    sound_segment.export(output_background_path, format=\"wav\")\n",
    "    \n",
    "    # Generate AHAP by background sound\n",
    "    ga.convert_wav_to_ahap(output_background_path, os.path.dirname(output_background_path), \"sfx\", \"none\")\n",
    "    print(\"New AHAP file generated successfully.\")\n",
    "    \n",
    "    audio_segment = adjust_audio_duration(audio_segment, duration_seconds)\n",
    "    audio_segment = overlay_background_sound(audio_segment, output_background_path)\n",
    "\n",
    "    # Export the final audio\n",
    "    audio_segment.export(output_path, format=\"wav\")\n",
    "    \n",
    "\"\"\"\n",
    "    # Load the AHAP file and modify with new merged audio\n",
    "    with open(sound_file_path.split('.', 1)[0] + '_combined.ahap', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Define the new event to be added\n",
    "    new_event = {\n",
    "        \"Event\": {\n",
    "            \"Time\": 0.0,\n",
    "            \"EventType\":\"AudioCustom\",\n",
    "            \"EventWaveformPath\":output_path,\n",
    "              \"EventParameters\":\n",
    "              [\n",
    "                  {\"ParameterID\":\"AudioVolume\",\"ParameterValue\":0.75}\n",
    "              ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Append the new event to the \"Pattern\" list\n",
    "    data[\"Pattern\"].append(new_event)\n",
    "\n",
    "    # Save the modified JSON data back to the file\n",
    "    with open(sound_file_path.split('.', 1)[0] + '_combined.ahap', 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    \n",
    "    print(\"New event added successfully.\")\n",
    "\n",
    "\n",
    "    # Overlay background sounds based on detected keywords\n",
    "    for keyword in detected_keywords:\n",
    "        background_file = keywords_to_sounds[keyword]\n",
    "        print(background_file)\n",
    "        audio_segment = overlay_background_sound(audio_segment, background_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "437e9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_path: /Users/user/Downloads/embrace_audio/Output.wav\n",
      "Successfully adjusted audio.\n",
      "Successfully adjusted audio.\n",
      "Successfully adjusted audio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|████████████████████████████████████| 545/545 [00:08<00:00, 65.93it/s]\n",
      "Processing continuous events: 100%|██████████████████████████████████| 600/600 [00:02<00:00, 293.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 40.49 seconds.\n",
      "Generated files:\n",
      " - /Users/user/Downloads/embrace_audio/Output-background_combined.ahap\n",
      "New AHAP file generated successfully.\n",
      "Successfully adjusted audio.\n"
     ]
    }
   ],
   "source": [
    "#file_path=\"/Users/user/Downloads/Mountain.mp3\",\n",
    "#output_path=\"/Users/user/Downloads/audio_output/output.wav\",\n",
    "\n",
    "keywords_to_sounds = {\n",
    "    \"focus\": \"/Users/user/Downloads/audio_output/sounds/birds-singing.mp3\",\n",
    "    \"sitting\": \"/Users/user/Downloads/audio_output/sounds/heavy-rain.mp3\",\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "process_audio(\n",
    "    speech_file_path=\"/Users/user/Downloads/embrace_audio/Ana.wav\",\n",
    "    sound_file_paths=[\"/Users/user/Downloads/embrace_audio/Waves.wav\",\n",
    "                    \"/Users/user/Downloads/embrace_audio/Thunder.wav\",\n",
    "                     \"/Users/user/Downloads/embrace_audio/Canon.mp3\"],\n",
    "    duration_seconds=60,\n",
    "    pitch_steps=0,               # Shift up 2 semitones\n",
    "    time_stretch_rate=1,      # Slow down by xx%\n",
    "    amp_change=5,                # Increase volume by 5 dB\n",
    "    eq_settings=(-3, 0, 3),       # Low gain: -3 dB, Mid gain: 0 dB, High gain: +3 dB\n",
    "    keywords_to_sounds=keywords_to_sounds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5bce61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytho3.11 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
