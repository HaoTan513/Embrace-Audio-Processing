{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a023c596",
   "metadata": {},
   "source": [
    "## Audio Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d619e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def convert_wav_to_ahap(input_wav, output_dir, mode, split):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load audio file using pydub\n",
    "        audio = AudioSegment.from_file(input_wav, format=os.path.splitext(input_wav)[-1][1:])\n",
    "\n",
    "        # Convert to mono and set sample rate to 44.1 kHz\n",
    "        audio = audio.set_channels(1).set_frame_rate(48000)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        audio_data = np.array(audio.get_array_of_samples())\n",
    "\n",
    "        # Convert to float32 in the range [-1, 1]\n",
    "        audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "\n",
    "        sample_rate = audio.frame_rate\n",
    "        duration = len(audio_data) / sample_rate\n",
    "\n",
    "        # Perform HPSS once\n",
    "        harmonic, percussive = librosa.effects.hpss(audio_data)\n",
    "\n",
    "        # Isolate bass using a low-pass filter\n",
    "        bass = librosa.effects.hpss(audio_data, margin=(1.0, 20.0))[0]\n",
    "\n",
    "        # Use the directory of the input WAV file if output_dir is not provided\n",
    "        if not output_dir:\n",
    "            output_dir = os.path.dirname(input_wav)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_files = []\n",
    "\n",
    "        if split == \"none\":\n",
    "            ahap_data = generate_ahap(audio_data, sample_rate, mode, harmonic, percussive, bass, duration, split)\n",
    "            output_ahap = os.path.join(output_dir, os.path.basename(input_wav).replace(os.path.splitext(input_wav)[-1], '.ahap'))\n",
    "            write_ahap_file(output_ahap, ahap_data)\n",
    "            output_files.append(output_ahap)\n",
    "        else:\n",
    "            splits = ['bass', 'vocals', 'drums', 'other']\n",
    "            for split_type in splits:\n",
    "                if split != \"all\" and split != split_type:\n",
    "                    continue\n",
    "                ahap_data = generate_ahap(audio_data, sample_rate, mode, harmonic, percussive, bass, duration, split_type)\n",
    "                output_ahap = os.path.join(output_dir, os.path.basename(input_wav).replace(os.path.splitext(input_wav)[-1], f'_{split_type}.ahap'))\n",
    "                write_ahap_file(output_ahap, ahap_data)\n",
    "                output_files.append(output_ahap)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"AHAP files generated successfully in {elapsed_time:.2f} seconds.\")\n",
    "        print(\"Generated files:\")\n",
    "        for file in output_files:\n",
    "            print(f\" - {file}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "def write_ahap_file(output_ahap, ahap_data):\n",
    "    # Write AHAP content to file\n",
    "    with open(output_ahap, 'w') as f:\n",
    "        json.dump(ahap_data, f, indent=4)\n",
    "\n",
    "def generate_ahap(audio_data, sample_rate, mode, harmonic, percussive, bass, duration, split):\n",
    "    \"\"\"\n",
    "    Generate AHAP content with both transient and continuous events.\n",
    "    \"\"\"\n",
    "    pattern = []\n",
    "\n",
    "    # Detect onsets for transients\n",
    "    onsets = librosa.onset.onset_detect(y=audio_data, sr=sample_rate)\n",
    "\n",
    "    # Convert onsets to time\n",
    "    event_times = librosa.frames_to_time(onsets, sr=sample_rate)\n",
    "    \n",
    "    # Plot the audio waveform, components, and event markers\n",
    "    ## Uncomment it only if you want to see the plot of the audio data\n",
    "    # plot_audio_components(audio_data, harmonic, percussive, bass, event_times, sample_rate)\n",
    "\n",
    "    # Create progress bar for transient events\n",
    "    with tqdm(total=len(event_times), desc=\"Processing transient events\") as pbar:\n",
    "        for time in event_times:\n",
    "            # Determine event type based on audio features\n",
    "            haptic_mode = determine_haptic_mode(audio_data, time, sample_rate, mode, harmonic, percussive, bass)\n",
    "            if haptic_mode in ['transient', 'both']:\n",
    "                event = create_event(\"HapticTransient\", time, audio_data, sample_rate, split)\n",
    "                pattern.append(event)\n",
    "            if haptic_mode in ['continuous', 'both']:\n",
    "                event = create_event(\"HapticContinuous\", time, audio_data, sample_rate, split)\n",
    "                pattern.append(event)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Add continuous events for bass and harmonic components\n",
    "    add_continuous_events(pattern, audio_data, sample_rate, harmonic, bass, duration, split)\n",
    "\n",
    "    ahap_data = {\"Version\": 1.0, \"Pattern\": pattern}\n",
    "    return ahap_data\n",
    "\n",
    "def create_event(event_type, time, audio_data, sample_rate, split):\n",
    "    \"\"\"\n",
    "    Create an event with appropriate parameters based on event type and audio features.\n",
    "    \"\"\"\n",
    "    intensity, sharpness = calculate_parameters(audio_data, time, sample_rate, split)\n",
    "    event = {\n",
    "        \"Event\": {\n",
    "            \"Time\": float(time),\n",
    "            \"EventType\": event_type,\n",
    "            \"EventParameters\": [\n",
    "                {\"ParameterID\": \"HapticIntensity\", \"ParameterValue\": float(intensity)},\n",
    "                {\"ParameterID\": \"HapticSharpness\", \"ParameterValue\": float(sharpness)}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    if event_type == \"HapticContinuous\":\n",
    "        event[\"Event\"][\"EventDuration\"] = 0.05  # Adjust duration as needed\n",
    "    return event\n",
    "\n",
    "def determine_haptic_mode(audio_data, time, sample_rate, mode, harmonic, percussive, bass):\n",
    "    \"\"\"\n",
    "    Determine whether to use transient, continuous, or both haptic modes based on audio features.\n",
    "    \"\"\"\n",
    "    # Calculate RMS energy in a small window around the specified time\n",
    "    window_size = int(sample_rate * 0.02)  # 20 ms window\n",
    "    start_index = max(0, int((time - 0.01) * sample_rate))  # Start 10 ms before the specified time\n",
    "    end_index = min(len(audio_data), start_index + window_size)\n",
    "    energy = np.sqrt(np.mean(audio_data[start_index:end_index] ** 2))\n",
    "\n",
    "    # Calculate sub-band energies using pre-computed harmonic, percussive, and bass components\n",
    "    bass_energy = np.sqrt(np.mean(bass[start_index:end_index] ** 2))\n",
    "    percussive_energy = np.sqrt(np.mean(percussive[start_index:end_index] ** 2))\n",
    "    harmonic_energy = np.sqrt(np.mean(harmonic[start_index:end_index] ** 2))\n",
    "\n",
    "    # Calculate spectral centroid in a small window around the specified time\n",
    "    window_size = int(sample_rate * 0.05)  # 50 ms window\n",
    "    start_index = max(0, int((time - 0.025) * sample_rate))  # Start 25 ms before the specified time\n",
    "    end_index = min(len(audio_data), start_index + window_size)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(\n",
    "        y=audio_data[start_index:end_index], sr=sample_rate\n",
    "    )\n",
    "\n",
    "    # Calculate additional features\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio_data[start_index:end_index])\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data[start_index:end_index], sr=sample_rate)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data[start_index:end_index], sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "    # Get mean value of spectral centroid for comparison\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    zcr_mean = np.mean(zcr)\n",
    "    spectral_rolloff_mean = np.mean(spectral_rolloff)\n",
    "    mfcc_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Adjust thresholds based on the mode\n",
    "    if mode == 'sfx':\n",
    "        transient_rms_threshold = 0.5\n",
    "        continuous_rms_threshold = 0.2\n",
    "        spectral_threshold = np.percentile(spectral_centroid, 90)\n",
    "    else:  # music\n",
    "        transient_rms_threshold = 0.2\n",
    "        continuous_rms_threshold = 0.1\n",
    "        spectral_threshold = np.percentile(spectral_centroid, 70)\n",
    "\n",
    "    # Classify based on a combination of features\n",
    "    if energy > transient_rms_threshold and spectral_centroid_mean > spectral_threshold:\n",
    "        return 'transient'\n",
    "    elif energy < continuous_rms_threshold:\n",
    "        return 'continuous'\n",
    "    else:\n",
    "        return 'both'\n",
    "\n",
    "def calculate_parameters(audio_data, time, sample_rate, split):\n",
    "    # Calculate RMS energy in a small window around the specified time\n",
    "    window_size = int(sample_rate * 0.02)  # 20 ms window\n",
    "    start_index = max(0, int((time - 0.01) * sample_rate))  # Start 10 ms before the specified time\n",
    "    end_index = min(len(audio_data), start_index + window_size)\n",
    "    energy = np.sqrt(np.mean(audio_data[start_index:end_index] ** 2))\n",
    "\n",
    "    # Calculate spectral centroid in a small window around the specified time\n",
    "    window_size = int(sample_rate * 0.05)  # 50 ms window\n",
    "    start_index = max(0, int((time - 0.025) * sample_rate))  # Start 25 ms before the specified time\n",
    "    end_index = min(len(audio_data), start_index + window_size)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(\n",
    "        y=audio_data[start_index:end_index], sr=sample_rate\n",
    "    )\n",
    "\n",
    "    # Calculate sharpness based on the spectral centroid\n",
    "    sharpness = np.mean(spectral_centroid)\n",
    "\n",
    "    # Scale the energy to the range [0, 1]\n",
    "    scaled_energy = np.clip(energy / np.max(audio_data), 0, 1)\n",
    "\n",
    "    # Increase the overall intensity to add more \"oomph\"\n",
    "    scaled_energy *= 3 ## By Siming\n",
    "    scaled_energy = np.clip(scaled_energy, 0, 1)\n",
    "\n",
    "    # Scale sharpness to a range that fits the haptic feedback parameters\n",
    "    scaled_sharpness = np.clip(sharpness / np.max(spectral_centroid) , 0, 1) * 0\n",
    "\n",
    "    # Adjust parameters based on split type\n",
    "    if split == \"vocal\":\n",
    "        scaled_energy *= 1.2\n",
    "        scaled_sharpness *= 1.1\n",
    "    elif split == \"drums\":\n",
    "        scaled_energy *= 1.5\n",
    "        scaled_sharpness *= 1.3\n",
    "    elif split == \"bass\":\n",
    "        scaled_energy *= 1.4\n",
    "        scaled_sharpness *= 0.9\n",
    "    elif split == \"other\":\n",
    "        scaled_energy *= 1.3\n",
    "        scaled_sharpness *= 1.2\n",
    "\n",
    "    return scaled_energy, scaled_sharpness\n",
    "\n",
    "def add_continuous_events(pattern, audio_data, sample_rate, harmonic, bass, duration, split):\n",
    "    \"\"\"\n",
    "    Add continuous haptic events for bass and harmonic components.\n",
    "    \"\"\"\n",
    "    time_step = 0.05  # Adjust time step for continuous events\n",
    "    num_steps = int(duration / time_step)\n",
    "    \n",
    "    # Create progress bar for continuous events\n",
    "    with tqdm(total=num_steps, desc=\"Processing continuous events\") as pbar:\n",
    "        for t in np.arange(0, duration, time_step):\n",
    "            bass_energy = np.sqrt(np.mean(bass[int(t * sample_rate):int((t + time_step) * sample_rate)] ** 2))\n",
    "            harmonic_energy = np.sqrt(np.mean(harmonic[int(t * sample_rate):int((t + time_step) * sample_rate)] ** 2))\n",
    "            \n",
    "            scaled_intensity = 3\n",
    "            scaled_sharpness = 0\n",
    "\n",
    "            # Calculate intensity and sharpness\n",
    "            intensity = np.clip(bass_energy / np.max(bass), 0, 1) * scaled_intensity\n",
    "            intensity = np.clip(intensity, 0, 1)\n",
    "            sharpness = np.clip(harmonic_energy / np.max(harmonic), 0, 1) * scaled_sharpness\n",
    "            \n",
    "            event = {\n",
    "                \"Event\": {\n",
    "                    \"Time\": float(t),\n",
    "                    \"EventType\": \"HapticContinuous\",\n",
    "                    \"EventDuration\": time_step,\n",
    "                    \"EventParameters\": [\n",
    "                        {\"ParameterID\": \"HapticIntensity\", \"ParameterValue\": float(intensity)},\n",
    "                        {\"ParameterID\": \"HapticSharpness\", \"ParameterValue\": float(sharpness)}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            pattern.append(event)\n",
    "            pbar.update(1)\n",
    "\n",
    "def plot_audio_components(audio_data, harmonic, percussive, bass, event_times, sample_rate):\n",
    "    \"\"\"\n",
    "    Plot the original audio waveform, harmonic, percussive, and bass components,\n",
    "    and mark detected events on the waveform.\n",
    "    \"\"\"\n",
    "    # Create a time axis for the waveform\n",
    "    time_axis = np.arange(len(audio_data)) / sample_rate\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "\n",
    "    # Plot the original waveform\n",
    "    plt.subplot(4, 1, 1)\n",
    "    librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "    plt.title(\"Original Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Plot the harmonic component\n",
    "    plt.subplot(4, 1, 2)\n",
    "    librosa.display.waveshow(harmonic, sr=sample_rate)\n",
    "    plt.title(\"Harmonic Component\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Plot the percussive component\n",
    "    plt.subplot(4, 1, 3)\n",
    "    librosa.display.waveshow(percussive, sr=sample_rate)\n",
    "    plt.title(\"Percussive Component\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Plot the bass component\n",
    "    plt.subplot(4, 1, 4)\n",
    "    librosa.display.waveshow(bass, sr=sample_rate)\n",
    "    plt.title(\"Bass Component\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    # Mark the event times on the original waveform\n",
    "    for event_time in event_times:\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.axvline(x=event_time, color='red', linestyle='--', label='Event Time' if event_time == event_times[0] else \"\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf34a0d-a482-4cee-997e-f548a771dd39",
   "metadata": {},
   "source": [
    "## Script for Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70462af7-0b1c-4369-8dea-12d427588cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def process_meditation_script_from_csv(file_path, output_path, voice_name=\"en-US-NancyNeural\", prosody_rate=\"-15.00%\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, processes the `original_content` column to generate SSML content,\n",
    "    and saves the result to a new CSV file with an additional column `ssml_content`.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the input CSV file.\n",
    "    - output_path (str): Path to save the output CSV file.\n",
    "    - voice_name (str): The name of the Azure Neural voice to use.\n",
    "    - prosody_rate (str): The prosody rate adjustment for the voice.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Function to convert a single chunk into SSML\n",
    "    def process_chunk_to_ssml(chunk):\n",
    "        # Replace periods with 2-second breaks\n",
    "        chunk = re.sub(r\"\\.\", r'.<break time=\"2s\"/>', chunk)\n",
    "        # Replace commas with 1-second breaks\n",
    "        chunk = re.sub(r\",\", r',<break time=\"1s\"/>', chunk)\n",
    "        # Replace [x's] with x-second breaks\n",
    "        chunk = re.sub(r\"\\[(\\d+)s\\]\", r'<break time=\"\\1s\"/>', chunk)\n",
    "        # Wrap with SSML structure\n",
    "        return f\"\"\"\n",
    "<speak xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "       xmlns:mstts=\"http://www.w3.org/2001/mstts\" \n",
    "       xmlns:emo=\"http://www.w3.org/2009/10/emotionml\" \n",
    "       version=\"1.0\" xml:lang=\"en-US\">\n",
    "  <voice name=\"{voice_name}\">\n",
    "    <mstts:express-as style=\"whispering\">\n",
    "      <prosody rate=\"{prosody_rate}\">\n",
    "        {chunk}\n",
    "      </prosody>\n",
    "    </mstts:express-as>\n",
    "  </voice>\n",
    "</speak>\n",
    "        \"\"\".strip()\n",
    "\n",
    "    # Process the `original_content` column\n",
    "    df['ssml_content'] = df['original_content'].apply(process_chunk_to_ssml)\n",
    "\n",
    "    # Save the DataFrame with the new column to a new CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Processed file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f69b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to: ssml_content_database.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_meditation_script_from_csv(\n",
    "    file_path=\"audio_content_database.csv\",\n",
    "    output_path=\"ssml_content_database.csv\",\n",
    "    voice_name=\"en-US-NancyNeural\",\n",
    "    prosody_rate=\"-15.00%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8ca99",
   "metadata": {},
   "source": [
    "## Genearte AI audio Along with Haptics and Soundscaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f3559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "seed = 17\n",
    "\n",
    "def synthesize_text_to_audio(ssml_text, speech_config):\n",
    "    \"\"\"\n",
    "    Synthesizes SSML text to an audio segment using Azure TTS.\n",
    "    \"\"\"\n",
    "    audio_config = speechsdk.audio.PullAudioOutputStream()\n",
    "    print(\"Attributes of audio_config: \", vars(audio_config))\n",
    "    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Synthesize the SSML\n",
    "    synthesis_result = synthesizer.speak_ssml_async(ssml_text).get()\n",
    "\n",
    "    if synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"SynthesizingAudioCompleted\")\n",
    "        audio_data = synthesis_result.audio_data\n",
    "        return AudioSegment.from_file(BytesIO(audio_data), format=\"wav\")\n",
    "    elif synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = synthesis_result.cancellation_details\n",
    "        print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "        return None\n",
    "\n",
    "def overlay_background_sound(speech_audio, sound_library_path, background_tag, delay = 1000, volume=-15):\n",
    "    \"\"\"\n",
    "    Overlays a background sound onto a speech audio segment.\n",
    "    \"\"\"\n",
    "    # Check for both .mp3 and .wav files\n",
    "    background_mp3 = os.path.join(sound_library_path, f\"{background_tag}.mp3\")\n",
    "    background_wav = os.path.join(sound_library_path, f\"{background_tag}.wav\")\n",
    "    \n",
    "    # Determine which file exists\n",
    "    if os.path.exists(background_mp3):\n",
    "        background_file = background_mp3\n",
    "    elif os.path.exists(background_wav):\n",
    "        background_file = background_wav\n",
    "    else:\n",
    "        print(f\"Background file {background_tag} not found in {sound_library_path}.\")\n",
    "        return speech_audio, None\n",
    "    \n",
    "    # Load the background audio and adjust its volume\n",
    "    background_audio = AudioSegment.from_file(background_file).apply_gain(volume)\n",
    "    \n",
    "    # Add silence to the beginning of the background audio (one-time delay)\n",
    "    delayed_background = AudioSegment.silent(duration=delay) + background_audio\n",
    "\n",
    "    # Extend the background audio to match the speech audio's length\n",
    "    loops_needed = (len(speech_audio) - len(delayed_background)) // len(background_audio) + 1\n",
    "    extended_background = delayed_background + (background_audio * loops_needed)\n",
    "    \n",
    "    # Trim the background to match the length of the speech audio (no looping)\n",
    "    final_background = extended_background[:len(speech_audio)]\n",
    "\n",
    "    return speech_audio.overlay(final_background), final_background\n",
    "\n",
    "def generate_ahap_file(background_file, output_dir=\"ahap_outputs\"):\n",
    "    \"\"\"\n",
    "    Generates an AHAP file from the provided background audio file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    ahap_output_file = os.path.join(output_dir, os.path.splitext(os.path.basename(background_file))[0] + \".ahap\")\n",
    "    convert_wav_to_ahap(background_file, output_dir, \"sfx\", \"none\")\n",
    "    print(f\"AHAP file saved as '{ahap_output_file}'.\")\n",
    "    return ahap_output_file\n",
    "\n",
    "def process_csv_and_generate_audio_with_ahap(csv_file_path,\n",
    "                                             sound_library_path, \n",
    "                                             target_function, \n",
    "                                             target_scenario, \n",
    "                                             background_delay = 5000):\n",
    "    \"\"\"\n",
    "    Processes a CSV file containing SSML text and sound tags, synthesizes audio,\n",
    "    adds background sounds, generates AHAP files, and combines the results.\n",
    "    \"\"\"\n",
    "    ## Sound library append\n",
    "    sound_library_path = os.path.join(sound_library_path, target_scenario)\n",
    "\n",
    "    # Load the CSV\n",
    "    data = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "    # Randomly select slices\n",
    "    # Filter the data for the target function\n",
    "    data = data[data['function'] == target_function]\n",
    "    data = data[data['scenario'] == target_scenario]\n",
    "\n",
    "    # Select the slices for each section (entry, body, end)\n",
    "    entry_slices = data[data['position'].str.contains('entry', na=False)].sort_values(by='position')\n",
    "    body_slices =  data[data['position'].str.contains('body', na=False)].sort_values(by='position')\n",
    "    end_slice = data[data['position'] == 'end']\n",
    "\n",
    "    # Combine slices into one dataframe\n",
    "    all_slices = pd.concat([entry_slices, body_slices, end_slice])\n",
    "\n",
    "    if all_slices.empty:\n",
    "        print(\"No slices found for the specified function.\")\n",
    "        return None\n",
    "\n",
    "    # Create output directories\n",
    "    output_dir = f\"{target_function}/{target_scenario}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize Azure TTS config\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=os.getenv(\"SPEECH_KEY\"),\n",
    "        region=os.getenv(\"SPEECH_REGION\")\n",
    "    )\n",
    "    ## Default is 48Khz with the highest quality\n",
    "    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff48Khz16BitMonoPcm)\n",
    "\n",
    "    # Process each slice\n",
    "    for index, row in all_slices.iterrows():\n",
    "        slice_name = f\"{row['position']}\"\n",
    "        ssml_text = row['ssml_content']\n",
    "        setting_tag = row['setting_tag']\n",
    "        event_tag = row['event_tag']\n",
    "\n",
    "        print(f\"Processing {slice_name} with setting tag '{setting_tag}' and event tag {event_tag}\")\n",
    "\n",
    "        # Synthesize SSML text to audio\n",
    "        speech_audio = synthesize_text_to_audio(ssml_text, speech_config)\n",
    "        if speech_audio is None:\n",
    "            continue\n",
    "\n",
    "        # Adjust background and incorporate event_tag\n",
    "        if pd.notna(event_tag):\n",
    "            print(f\"Event '{event_tag}' detected. Lowering background volume and adding event.\")\n",
    "            # Overlay background (sound_tag) and event sound\n",
    "            speech_audio, background_sound = overlay_background_sound(speech_audio, sound_library_path, event_tag, background_delay ,volume=-5)\n",
    "            speech_audio, _ = overlay_background_sound(speech_audio, sound_library_path, setting_tag, background_delay, volume=-5)\n",
    "        else:\n",
    "            print(\"No event detected. Using default background.\")\n",
    "            # Default background overlay\n",
    "            speech_audio, background_sound = overlay_background_sound(speech_audio, sound_library_path, setting_tag, background_delay,volume=-5)\n",
    "\n",
    "\n",
    "        # Export audio file\n",
    "        audio_output_path = os.path.join(output_dir, f\"{slice_name}.wav\")\n",
    "        speech_audio.export(audio_output_path, format=\"wav\")\n",
    "        print(f\"Audio file saved as '{audio_output_path}'.\")\n",
    "\n",
    "        # Export Background file \n",
    "        background_name = slice_name + \"_background\"\n",
    "        background_output_path = os.path.join(output_dir, f\"{background_name}.wav\")\n",
    "        background_sound.export(background_output_path, format = \"wav\")\n",
    "        print(f\"Background file saved as '{background_output_path}'.\")\n",
    "\n",
    "        # Generate AHAP file\n",
    "        ahap_output_path = generate_ahap_file(background_output_path, output_dir=output_dir)\n",
    "        print(f\"AHAP file saved as '{ahap_output_path}'.\")\n",
    "\n",
    "    print(\"All slices processed successfully.\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e81e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entry1 with setting tag 'leaves' and event tag walk_on_gravel\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x34552e390>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'walk_on_gravel' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/zengarden/entry1.wav'.\n",
      "Background file saved as 'meditate/zengarden/entry1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 702/702 [00:03<00:00, 200.02it/s]\n",
      "Processing continuous events: 1260it [00:00, 1719.51it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 14.24 seconds.\n",
      "Generated files:\n",
      " - meditate/zengarden/entry1_background.ahap\n",
      "AHAP file saved as 'meditate/zengarden/entry1_background.ahap'.\n",
      "AHAP file saved as 'meditate/zengarden/entry1_background.ahap'.\n",
      "Processing entry2 with setting tag 'zengarden' and event tag nan\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x329889210>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "No event detected. Using default background.\n",
      "Audio file saved as 'meditate/zengarden/entry2.wav'.\n",
      "Background file saved as 'meditate/zengarden/entry2_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 794/794 [00:03<00:00, 249.23it/s]\n",
      "Processing continuous events: 3422it [00:04, 716.37it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 36.70 seconds.\n",
      "Generated files:\n",
      " - meditate/zengarden/entry2_background.ahap\n",
      "AHAP file saved as 'meditate/zengarden/entry2_background.ahap'.\n",
      "AHAP file saved as 'meditate/zengarden/entry2_background.ahap'.\n",
      "Processing body1 with setting tag 'zengarden' and event tag zen_fountain_ambience\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x329ac0ed0>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'zen_fountain_ambience' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/zengarden/body1.wav'.\n",
      "Background file saved as 'meditate/zengarden/body1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 251/251 [00:01<00:00, 157.53it/s]\n",
      "Processing continuous events: 7284it [00:19, 381.19it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 85.44 seconds.\n",
      "Generated files:\n",
      " - meditate/zengarden/body1_background.ahap\n",
      "AHAP file saved as 'meditate/zengarden/body1_background.ahap'.\n",
      "AHAP file saved as 'meditate/zengarden/body1_background.ahap'.\n",
      "Processing body2 with setting tag 'zengarden' and event tag singing_bowl\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x328b86ed0>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'singing_bowl' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/zengarden/body2.wav'.\n",
      "Background file saved as 'meditate/zengarden/body2_background.wav'.\n"
     ]
    }
   ],
   "source": [
    "process_csv_and_generate_audio_with_ahap('ssml_content_database.csv', \n",
    "                                         '../../sound_library',\n",
    "                                         target_function=\"meditate\", \n",
    "                                         target_scenario=\"zengarden\", \n",
    "                                         background_delay= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952aa87",
   "metadata": {},
   "source": [
    "## generate all functions and scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entry1 with setting tag 'grass' and event tag nan\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x345c0cf50>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "No event detected. Using default background.\n",
      "Audio file saved as 'meditate/beach/entry1.wav'.\n",
      "Background file saved as 'meditate/beach/entry1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 322/322 [00:00<00:00, 325.38it/s]\n",
      "Processing continuous events: 875it [00:00, 2568.40it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 8.26 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/entry1_background.ahap\n",
      "AHAP file saved as 'meditate/beach/entry1_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/entry1_background.ahap'.\n",
      "Processing entry2 with setting tag 'seagull' and event tag wave1\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x345b05ed0>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'wave1' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/beach/entry2.wav'.\n",
      "Background file saved as 'meditate/beach/entry2_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 341/341 [00:01<00:00, 271.85it/s]\n",
      "Processing continuous events: 2378it [00:02, 1031.70it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 22.45 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/entry2_background.ahap\n",
      "AHAP file saved as 'meditate/beach/entry2_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/entry2_background.ahap'.\n",
      "Processing body1 with setting tag 'seagull' and event tag wave1\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x32c035d10>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'wave1' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/beach/body1.wav'.\n",
      "Background file saved as 'meditate/beach/body1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 880/880 [00:03<00:00, 221.33it/s]\n",
      "Processing continuous events: 6276it [00:15, 414.44it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 73.03 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/body1_background.ahap\n",
      "AHAP file saved as 'meditate/beach/body1_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/body1_background.ahap'.\n",
      "Processing body2 with setting tag 'wave1' and event tag bird\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x1239b2350>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'bird' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/beach/body2.wav'.\n",
      "Background file saved as 'meditate/beach/body2_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 2181/2181 [00:16<00:00, 135.69it/s]\n",
      "Processing continuous events: 9565it [00:38, 250.39it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 140.00 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/body2_background.ahap\n",
      "AHAP file saved as 'meditate/beach/body2_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/body2_background.ahap'.\n",
      "Processing body3 with setting tag 'wave1' and event tag campfire\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x32c7ab9d0>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'campfire' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/beach/body3.wav'.\n",
      "Background file saved as 'meditate/beach/body3_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 2919/2919 [00:13<00:00, 213.17it/s]\n",
      "Processing continuous events: 6140it [00:16, 379.41it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 82.33 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/body3_background.ahap\n",
      "AHAP file saved as 'meditate/beach/body3_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/body3_background.ahap'.\n",
      "Processing end with setting tag 'seagull' and event tag grass\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x1682a7650>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'grass' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/beach/end.wav'.\n",
      "Background file saved as 'meditate/beach/end_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 491/491 [00:01<00:00, 291.71it/s]\n",
      "Processing continuous events: 1347it [00:00, 1459.21it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 13.14 seconds.\n",
      "Generated files:\n",
      " - meditate/beach/end_background.ahap\n",
      "AHAP file saved as 'meditate/beach/end_background.ahap'.\n",
      "AHAP file saved as 'meditate/beach/end_background.ahap'.\n",
      "All slices processed successfully.\n",
      "Processing entry1 with setting tag 'bird' and event tag grass\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x34fc48a50>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'grass' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/forest/entry1.wav'.\n",
      "Background file saved as 'meditate/forest/entry1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 399/399 [00:01<00:00, 299.97it/s]\n",
      "Processing continuous events: 1062it [00:00, 1851.59it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 10.18 seconds.\n",
      "Generated files:\n",
      " - meditate/forest/entry1_background.ahap\n",
      "AHAP file saved as 'meditate/forest/entry1_background.ahap'.\n",
      "AHAP file saved as 'meditate/forest/entry1_background.ahap'.\n",
      "Processing entry2 with setting tag 'bird' and event tag nan\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x1680d7e50>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "No event detected. Using default background.\n",
      "Audio file saved as 'meditate/forest/entry2.wav'.\n",
      "Background file saved as 'meditate/forest/entry2_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 516/516 [00:01<00:00, 266.08it/s]\n",
      "Processing continuous events: 2014it [00:01, 1051.95it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 20.32 seconds.\n",
      "Generated files:\n",
      " - meditate/forest/entry2_background.ahap\n",
      "AHAP file saved as 'meditate/forest/entry2_background.ahap'.\n",
      "AHAP file saved as 'meditate/forest/entry2_background.ahap'.\n",
      "Processing body1 with setting tag 'bird' and event tag water_stream\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x32b45b590>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'water_stream' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/forest/body1.wav'.\n",
      "Background file saved as 'meditate/forest/body1_background.wav'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transient events: 100%|██████████| 108/108 [00:00<00:00, 224.81it/s]\n",
      "Processing continuous events: 6595it [00:16, 408.09it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHAP files generated successfully in 75.10 seconds.\n",
      "Generated files:\n",
      " - meditate/forest/body1_background.ahap\n",
      "AHAP file saved as 'meditate/forest/body1_background.ahap'.\n",
      "AHAP file saved as 'meditate/forest/body1_background.ahap'.\n",
      "Processing body2 with setting tag 'bird' and event tag leaves\n",
      "Attributes of audio_config:  {'_AudioOutputStream__handle': <azure.cognitiveservices.speech.interop._Handle object at 0x3288b6b10>}\n",
      "SynthesizingAudioCompleted\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Event 'leaves' detected. Lowering background volume and adding event.\n",
      "Audio file saved as 'meditate/forest/body2.wav'.\n",
      "Background file saved as 'meditate/forest/body2_background.wav'.\n"
     ]
    }
   ],
   "source": [
    "scenarios = [\"beach\", \"forest\", \"zengarden\"]\n",
    "functions = [\"meditate\", \"relax\", \"sleep\", \"oneminutereset\"]\n",
    "for function in functions:\n",
    "    for scenario in scenarios:\n",
    "        if function == \"beach\" and scenario == \"meditate\":\n",
    "            continue\n",
    "        process_csv_and_generate_audio_with_ahap('ssml_content_database.csv', \n",
    "                                                 '../../sound_library',\n",
    "                                                 target_function=function, \n",
    "                                                 target_scenario=scenario, \n",
    "                                                 background_delay= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1a83585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85XkVYuiMQ3hK1shqTMFVqArxcF7kYFyifxNLtt3nZscm7ofCP3QJQQJ99AKAC1i4TkXJ3w3AAAYACOGFPFv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"SPEECH_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9097ef86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12471f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
